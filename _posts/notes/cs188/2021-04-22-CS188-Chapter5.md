---
layout: post
title: CS188 Chapter 5 Adversarial Search
Author: Mark
tags: Notes CS188
---

### 1. Games

This chapter describes the **Competitive Environments** for agents, where their goals are conflict. Such problem is called the **adversarial search** problems - often known as **games**.

In the field of AI, the most common games are a special kind of game - the **Deterministic, Turn-taking, Two-player, Zero-sum games** of **Perfect Information** (such as chess).

Games are interesting since they are hard to solve using direct search - the branching factor of game is too large that it is impossible to search through all possible states. Also, games penalize inefficiency severely so the program should be as fast as possible.

**Pruning** （剪枝） allows us to ignore portions of search tree that make no difference to the final choice. Heuristic **Evaluation Functions** allow us to approximate the true utility of a state without doing a complete search.

Suppose there are two agents - "MAX" and "MIN". In a game, "MAX" move first and they take turn to move until the game is over. The winner get points and loser get penalty.

#### 1.1 Formally Defined Game

A game can be formally defined with these elements

| Element            | Explanation                                                  |
| ------------------ | ------------------------------------------------------------ |
| $S_0$              | The Initial State of game (the setup of game)                |
| $Player(s)$        | Defines which player has a move in the current state $s$     |
| $Actions(s)$       | Returns a list of legal actions in a state $s$               |
| $Result(s, a)$     | The *state transition model*, which defines the result of an action $a$ |
| $Terminal-Test(s)$ | Returns `true` if the game is over, `false` otherwise        |
| $Utility(s, p)$    | A utility function defines the numeric value for a game that ends in terminal state $s$ for player $p$ |

The **Zero-sum** game is defined as one where the <mark>total payoff to all players is the same for every instance of the game</mark>.

The initial state $S_0$, $Action$ function and $Result$ function define the **game tree** for the game - a tree where the nodes are game states and the edges are actions.

In a game tree, we record the utility value of the terminal state from the point of view of $MAX$ agent.

Though a game tree is well-defined and has finite amount of nodes in it, it is better thought to be a theoretical construct we can't realize in physical world as it has too many nodes in it (the Go has $10^40$ nodes, tic-tac-toe has more than $3\times 10^5$ nodes). We usually use term **search tree** to represent a tree that is extracted from the full game tree, and contains enough nodes to allow a player to determine what action to make.

### 2. Optimal Decision in Games

In an adversarial search scene, the MAX agent must find a contingent **strategy**. An optimal strategy leads to outcomes at least as good as other strategy when one is playing an infallible opponent.

![image-20210422204817496](https://gitee.com/MarkYutianChen/mark-markdown-imagebed/raw/master/20210422204817.png)

> In the graph above, the maximum utility at node $A$ is 3, since the MAX agent can't change the choice of MIN agent, and in the worst situation (no matter which action MAX agent choose, MIN will always select the successor with min utility for MAX), the maximum utility at $A$ is 3, when MAX takes action $a_1$.

In game theory, we combine a move of MAX and a move of MIN as "one move", and call two half-moves the "ply"

