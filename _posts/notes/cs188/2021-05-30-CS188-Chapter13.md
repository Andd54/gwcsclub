---
layout: post
title: CS188 Chapter 13 Quantifying Uncertainty
Author: Mark
tags: [Notes, CS188]
---

## 13.1 Acting Under Uncertainty

An intelligent agent in reality needs to handle the **uncertainty** due to partial observability and nondeterminism.

In previous chapters, agents handle uncertainty by keeping track of a **belief state** - a representation of the set of <u>all possible states that it might be in</u>.

![74c2def286635db980f08a29f1063bc](http://markdown-img-1304853431.cosgz.myqcloud.com/20210530155955.jpg)

Based on the belief states, a contingency plan that handles every possible eventuality that its sensors may report during execution will be generated.

![](http://markdown-img-1304853431.cosgz.myqcloud.com/20210530162342.jpg)

However, such method has significant draw back

* When interpreting partial sensor information, a logical agent must consider every *logically possible* explanation for the observations - this lead to **impossibly large and complex belief state representations**.
* A correct contingency plan handle every eventuality can grow arbitrarily large and must consider arbitrarily unlikely contingency.
* Sometimes, there does not exist a plan that is guaranteed to achieve the goal. However, agent should be able to compare the plans even if they are not guaranteed.

![789eb9ea9e270d655adce60dfe5763a](http://markdown-img-1304853431.cosgz.myqcloud.com/20210530162436.jpg)

If we are using the belief state + contingency plan technique to solve real-world problem, the intelligent agent will hardly generate a valid solution. Suppose we want our agent plan the schedule to airport, the agent may think - "we should leave at 9 a.m. , as long as the weather is good". Then, as the requirement of contingency plan is to **handle all possible situation**, it will begin to think - "what if the weather is bad", "what if the bridge falls down due to storm", "what if ..." and finally reach a solution - "you should leave 2 months earlier to the airport to get there on time".

So what's the problem with this method? By definition, we *should* leave 2 months earlier to make sure we can arrive at the airport on time in *any* situation, but the question is - do we really need such certainty? Do we need to get to airport on time even if there is an earthquake? This leads to the concept of **rational decision**.

<div class="info">
	<p><b>Rational Decision</b> - Rational decision depends on both the <b>relative importance</b> of goal and <b>the likelihood</b> that, and degree to which, they will be achieved.</p>
</div>
### 13.1.1 Summarizing uncertainty

Suppose an intelligent agent aims to get high GPA is deployed. The agent wants to list all the actions that will lead to low GPA:

$$
Low GPA \rightarrow Low Test Scode \vee AbsentCourse \vee PlayGames \cdots
$$

It is clear that it is an almost unlimited list of possible problems that can cause Low GPA. Some people may try to reverse this logical relationship:

$$
Low TestScore \rightarrow LowGPA
$$

Which, unfortunately is also false, as not all low test score will lead to low GPA. So there does not exist a clear and definite logical relationship between $LowTestScore$ and $LowGPA$ anyway. In this situation, we need to describe the relationship between them using **degree of belief**, and solve problems using **probability theory**.

> Probability provides a way of summarizing the uncertainty that comes from our laziness[^1] and ignorance[^2]

<mark>Probability statements are made with respect to a knowledge state, not with respect to the real world</mark>. When we say a student has a probability of $0.8$ to get low GPA, we are inference based on our knowledge state, not on the real world (as the student must either has high GPA or low GPA).

[^1]: It's too much work to list the complete set of antecedents or consequents needed to ensure an exceptionless rule and too hard to use such rules
[^2]: We don't have complete knowledge on most of the domain.

### 13.1.2 Uncertainty and Rational Decisions

Consider the airport schedule problem we mentioned above again.  What should the agent choose as the final result? The one that leave 2 months ago and 100% make sure you arrive on time, or the one that leave 3 hours before the plane take off?

To make such choices, an agent **must** first have **preferences** between the different possible **outcomes** of the various plans. We use the **utility theory** to represent and reason with preferences.

> **Utility Theory**: Every state has a degree of usefulness, or utility, to an agent and that the agent will prefer states with higher utility

$$
\text{Decision Theory} = \text{Probability Theory} + \text{Utility Theory}
$$

<div class="notification">
    <p>
        <b>Principle of Maximum Expected Utility (MEU)</b>: An agent is rational if and only if it chooses the action that yields the highest expected utility, averaged over all the possible outcomes of the action.
    </p>
</div>

![b3d157353fb5ae7becc43a063d70e65](http://markdown-img-1304853431.cosgz.myqcloud.com/20210530180125.jpg)

MEU is the statistical mean (expectation) of utility under best situation.

## 13.2 Probability Notation

