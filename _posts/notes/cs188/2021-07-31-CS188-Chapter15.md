---
layout: post
title: CS188 Chapter 15 Probabilistic Reasoning over Time
Author: ["Mark Chen"]
tags: [Notes, CS188]
---
<div class="error">
    <p>We are still working on this page ...</p>
</div>
<div class="info">
<em>In which we try to interpret the present, understand the past, and perhaps predict the future, even when very little is crystal clear.</em>
</div>

**belief state** that represents which states of the world are currently possible. From belief state and a **transition model**, the agent can predict how the world might evolve in the next step.

![How Belief State Evolves](http://markdown-img-1304853431.cosgz.myqcloud.com/20210731142523.jpg)

From information from the **sensor model**, agents can update the belief state.

![Use Sensor update Belief State](http://markdown-img-1304853431.cosgz.myqcloud.com/20210731142629.jpg)

In *previous* section, belief state can only tell which state is *possible*, but can't say which state is *likely* or *unlikely*. In this section, we can **quantify the degree of belief in the belief state**.

## 15.1 Time and Uncertainty

In previous section, we have talked about the Bayesian Network. Bayesian network can perform probabilistic reasoning in static world. The value of random variable at each moment is independent with its previous value.

However, in many scenario, the previous state **DO** influence the current state. To utilize such information, we use a model called **Markov chain**.

### 15.1.1 States and observations

We view the world as a series of snapshots (a.k.a. **time slices** and time is not continuous)

$\mathbf{X}_t$ denote the set of state variables at time $t$

$\mathbf{E}_t$ denote the set of observable evidence variables at time $t$

$\mathbf{E}_t = e_t$ where $e_t$​ is the observed value for evidence variable.

We will assume that the state sequence starts at $t=0$ and evidence starts arriving at $t=1$. 

### 15.1.2 Transition and Sensor Models

Transition model specifies the probability distribution over the latest state variables given previous values, that is, 

$$
\mathbf{P}\left(\mathbf{X}_t \mid \mathbf{X}_{0:t-1}\right)
$$

However, this leads to a problem: as $x\rightarrow \infty$, the size of set $\mathbf{X}_{0:t-1}$ has an unbounded size. To solve this problem, we introduce the **Markov Assumption**.

<div class="info" markdown=1>
**Markov Assumption**
Markov process is <em>memoryless</em>, which means that Current State only depends on a <em>finite fixed number</em> of previous states.
</div>

<u>Process satisfying markov assumption is called <b>Markov Process</b> or <b>Markov Chains</b>.</u>

<div class="info">
<b>First Order Markov Process</b> - $\mathbf{X}_{t}$ is only related with $\mathbf{X}_{t-1}$​

$$
\mathbf{P}(\mathbf{X}\mid \mathbf{X}_{0:t-1}) = \mathbf{P}(\mathbf{X}\mid \mathbf{X}_{t-1})
$$

</div>

Beyond first-order Markov process, there are second, third, ..., n-th order Markov Process.

**N-order Markov Process**

$$
\mathbf{P}(\mathbf{X}\mid \mathbf{X}_{0:t-1}) = \mathbf{P}(\mathbf{X}\mid \lbrace\mathbf{X}_{t-1}, \mathbf{X}_{t-2}, \cdots, \mathbf{X}_{t-n}\rbrace)
$$

Therefore, in a first-order Markov process, the transition model is the conditional distribution $\mathbf{P}(\mathbf{X_t}\mid \mathbf{X}_{t-1})$

Besides Markov Assumption, we also assume the world is *stationary*. At any given $t$, the transition model $\mathbf{P}(\mathbf{X_t}\mid \mathbf{X}_{t-1})$ is the same.

<div class="info">
<b>Stationary Assumption</b>

$$
\mathbf{P}(\mathbf{X_m}\mid \mathbf{X}_{m-1})\equiv \mathbf{P}(\mathbf{X_n}\mid \mathbf{X}_{n-1}) \forall m, n
$$

</div>

During the process, we can learn the status of world through **sensor**. The sensors read from world status and give us number. Therefore, it's safe to assume that 

<div class="info">
<b>Sensor Assumption</b>

$$
\mathbf{P}(\mathbf{E}_t\mid\mathbf{X}_{0:t}, \mathbf{E}_{0:t-1}) = \mathbf{P}(\mathbf{E}_t \mid \mathbf{X}_t)
$$

</div>

$\mathbf{P}(\mathbf{E}_t \mid \mathbf{X}_t)$ is the **Sensor Model**, sometimes also called *Observation Model*.

![bfe4f431a5fb0f8bbed1896c3512dc8](http://markdown-img-1304853431.cosgz.myqcloud.com/20210802183103.jpg)

### 15.1.3\* Improve Markov Model?

Generally, there are two methods to increase the precision of Markov Model:

* Increase the order of Markov Process Model
* Increase the set of state variables.

By increasing the order of model, we can use information from multiple ticks to calculate the probability distribution on time $t$.

Increasing the order of model can *always* be reformulated as an increase in the set of state variables.

## 15.2 Inference in Temporal Models

There are four basic inference tasks that must be solved in generic temporal model:

* **Filtering** - computing the *Belief State* - the posterior distribution over the most recent state - given all evidence to date.

  $$
    \mathbf{P}(\mathbf{X_t}\mid \mathbf{e}_{1:t})
  $$​

  With filtering, agent can keep track on current state and make rational decisions.

* **Prediction** - computing the posterior distribution over the future state, given all evidence to date. 

  $$
    \mathbf{P}(\mathbf{X_{t+k}}\mid \mathbf{e}_{1:t})
  $$

  Prediction is useful for evaluating possible courses of action based on their expected outcomes.

* **Smoothing** - computing the posterior distribution over a *past* state, given all evidence up to the present. 

  $$
    \mathbf{P}(\mathbf{X}_k\mid\mathbf{e}_{1:t}) , \text{where }1 \leq k\leq t
  $$

  Smoothing provides a better estimate of the state than was available at the time, because it incorporates more evidence.

* **Most likely explanation** - Given a sequence of observations, we might wish to find the sequence of states that is most likely to have generated those observations.

  $$
    \text{argmax}_{\mathbf{x}_{1:t}}P(\mathbf{x_{1:t}\mid \mathbf{e}_{1:t}})
  $$

